{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd   \n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import mean\n",
    "\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import learning_curve, train_test_split, validation_curve\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"dataset/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"dataset/train.csv\")\n",
    "\n",
    "# Deleted irrelevant columns from df1\n",
    "df1.drop(['RecordID'], axis=1, inplace=True)\n",
    "df1.drop(['hospital_id'], axis=1, inplace=True)\n",
    "df1.drop(['icu_id'], axis=1, inplace=True)\n",
    "df1.drop(['icu_stay_type'], axis=1, inplace=True)\n",
    "df1.drop(['ethnicity'], axis=1, inplace=True)\n",
    "df1.drop(['gender'], axis=1, inplace=True)\n",
    "df1.drop(['apache_3j_bodysystem'], axis=1, inplace=True)\n",
    "df1.drop(['apache_2_bodysystem'], axis=1, inplace=True)\n",
    "df1.drop(['icu_type'], axis=1, inplace=True)\n",
    "df1.drop(['icu_admit_source'], axis=1, inplace=True)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# df1['apache_3j_bodysystem'] = label_encoder.fit_transform(df1['apache_3j_bodysystem'])\n",
    "# df1['apache_2_bodysystem'] = label_encoder.fit_transform(df1['apache_2_bodysystem'])\n",
    "# df1['icu_type'] = label_encoder.fit_transform(df1['icu_type'])\n",
    "# df1['icu_admit_source'] = label_encoder.fit_transform(df1['icu_admit_source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_onehot = pd.get_dummies(df1)\n",
    "# df_onehot.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple imputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputed_data = imputer.fit_transform(df_onehot.values)\n",
    "X = imputed_data[:, :-1] \n",
    "y = imputed_data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN imputer\n",
    "knn_imputer = KNNImputer(n_neighbors=2, weights='uniform')\n",
    "imputed_data = knn_imputer.fit_transform(df_onehot.values)\n",
    "X = imputed_data[:, :-1] \n",
    "y = imputed_data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the distribution of y\n",
    "target_variable_counts = np.bincount(y.astype(int))\n",
    "print(\"Value counts of the target variable 'hospital_death':\")\n",
    "for value, count in enumerate(target_variable_counts):\n",
    "    print(f\"Value: {value}, Count: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEARNING CURVE FOR TRAINING AND TESTING VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipe_lr = make_pipeline(StandardScaler(),\n",
    "#                        LogisticRegression(penalty='l2', max_iter=10000))\n",
    "\n",
    "pipe_lr = Pipeline([('classifier', DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0))])\n",
    "\n",
    "train_sizes, train_scores, test_scores =\\\n",
    "                learning_curve(estimator=pipe_lr,\n",
    "                               X=trainX,\n",
    "                               y=trainy,\n",
    "                               train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "                               cv=10,\n",
    "                               n_jobs=1)\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_mean,\n",
    "         color='blue', marker='o',\n",
    "         markersize=5, label='Training accuracy')\n",
    "\n",
    "plt.fill_between(train_sizes,\n",
    "                 train_mean + train_std,\n",
    "                 train_mean - train_std,\n",
    "                 alpha=0.15, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, test_mean,\n",
    "         color='green', linestyle='--',\n",
    "         marker='s', markersize=5,\n",
    "         label='Validation accuracy')\n",
    "\n",
    "plt.fill_between(train_sizes,\n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std,\n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('Number of training examples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim([0.8, 1.03])\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging and Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, model_name):\n",
    "    model.fit(trainX,trainy)\n",
    "    md_probs = model.predict_proba(testX)\n",
    "    md_probs = md_probs[:,1]\n",
    "    md_auc = roc_auc_score(testy, md_probs)\n",
    "    print(model_name, \" : \", md_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_models = 300\n",
    "depth_level = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record the start time\n",
    "start_time = time.time()\n",
    "rf = RandomForestClassifier(max_depth=10,n_estimators=num_of_models)\n",
    "fit_model(rf, \"Random Forest\")\n",
    "#record the end time\n",
    "end_time = time.time()\n",
    "#calculate the total time\n",
    "total_time = end_time - start_time\n",
    "print(\"Total time RF: \", total_time)\n",
    "\n",
    "#record the start time\n",
    "start_time = time.time()\n",
    "gb = GradientBoostingClassifier(max_depth=depth_level,n_estimators=num_of_models)\n",
    "fit_model(gb, \"Graident Boosting\")\n",
    "#record the end time\n",
    "end_time = time.time()\n",
    "#calculate the total time\n",
    "total_time = end_time - start_time\n",
    "print(\"Total time GB: \", total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest with n_jobs=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record the start time\n",
    "start_time = time.time()\n",
    "rf = RandomForestClassifier(max_depth=10,n_estimators=num_of_models, n_jobs=-1 )\n",
    "fit_model(rf, \"Random Forest\")\n",
    "#record the end time\n",
    "end_time = time.time()\n",
    "#calculate the total time\n",
    "total_time = end_time - start_time\n",
    "print(\"Total time RF: \", total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADABOOST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record the start time\n",
    "start_time = time.time()\n",
    "ab = AdaBoostClassifier(n_estimators=num_of_models)\n",
    "fit_model(ab, \"Adaptive Boosting\")\n",
    "#record the end time\n",
    "end_time = time.time()\n",
    "#calculate the total time\n",
    "total_time = end_time - start_time\n",
    "print(\"Total time AB: \", total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XG BOOST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(max_depth=depth_level, n_estimators=num_of_models, learning_rate=0.1)\n",
    "start_time = time.time()\n",
    "#fit xgb_model\n",
    "xgb_model.fit(trainX,trainy)\n",
    "md_probs = xgb_model.predict_proba(testX)\n",
    "md_probs = md_probs[:,1]\n",
    "md_auc = roc_auc_score(testy, md_probs)\n",
    "print(\"XG Boost\", \" : \", md_auc)\n",
    "#record the end time\n",
    "end_time = time.time()\n",
    "#calculate the total time\n",
    "total_time = end_time - start_time\n",
    "print(\"Total time XGB: \", total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIGHT GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = lgb.LGBMClassifier(max_depth=depth_level, n_estimators=num_of_models, learning_rate=0.1, force_col_wise='true')\n",
    "start_time = time.time()\n",
    "#fit xgb_model\n",
    "lgb_model.fit(trainX,trainy)\n",
    "md_probs = lgb_model.predict_proba(testX)\n",
    "md_probs = md_probs[:,1]\n",
    "md_auc = roc_auc_score(testy, md_probs)\n",
    "print(\"LG Boost\", \" : \", md_auc)\n",
    "#record the end time\n",
    "end_time = time.time()\n",
    "#calculate the total time\n",
    "total_time = end_time - start_time\n",
    "print(\"Total time LGB: \", total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Define your dataset (X and y)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 1.0],\n",
    "    'reg_lambda': [0, 0.1, 1.0],\n",
    "    'boosting_type': ['gbdt', 'dart'],  # Specific to LightGBM\n",
    "    'num_leaves': [31, 63, 127],  # Specific to LightGBM\n",
    "    'min_child_samples': [1, 5, 10]  # Specific to LightGBM\n",
    "}\n",
    "\n",
    "# Create an XGBoost classifier\n",
    "lgbm_model = LGBMClassifier(device='gpu', device_id=0)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "  estimator=lgbm_model,\n",
    "  param_grid=param_grid,\n",
    "  scoring='accuracy',\n",
    "  cv=5,\n",
    "  n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(trainX,trainy)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train the final XGBoost model with the best hyperparameters\n",
    "final_model = LGBMClassifier(**best_params)\n",
    "final_model.fit(trainX,trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Simple Imputer\n",
    "\n",
    "df_test = pd.read_csv(\"dataset/test.csv\")\n",
    "\n",
    "df_test.drop(['RecordID', 'hospital_id', 'icu_id', 'icu_stay_type', 'ethnicity', 'gender', 'apache_3j_bodysystem', 'apache_2_bodysystem', 'icu_type', 'icu_admit_source'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "df_test_onehot = pd.get_dummies(df_test)\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X_test = imputer.fit_transform(df_test_onehot)\n",
    "\n",
    "record_ids = np.arange(50001, 80001)\n",
    "\n",
    "\n",
    "predictions = lgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "output_df = pd.DataFrame({'RecordID': record_ids, 'hospital_death': predictions})\n",
    "output_df.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLD STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_5_2 = DecisionTreeClassifier(max_depth=5, min_samples_leaf=10)  \n",
    "dt_5_2.fit(trainX,trainy)\n",
    "y_probs = dt_5_2.predict_proba(testX)[:,1]\n",
    "y_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_auc = roc_auc_score(testy, y_probs)\n",
    "md_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, model_name):\n",
    "    model.fit(trainX,trainy)\n",
    "    y_probs = model.predict_proba(testX)\n",
    "    y_probs = y_probs[:,1]\n",
    "    md_auc = roc_auc_score(testy, y_probs)\n",
    "    print(model_name, \" : \", md_auc)\n",
    "    md_fpr, md_tpr, _ = roc_curve(testy, y_probs)\n",
    "    pyplot.plot(md_fpr, md_tpr, marker='.', label=model_name)\n",
    "    #return (md_fpr, md_tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DT, GNB AND KNN (No longer to be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "dt_5_2 = DecisionTreeClassifier(max_depth=5)  \n",
    "fit_model(dt_5_2, \"Depth 5 Split 2\") \n",
    "\n",
    "dt_7_2 = DecisionTreeClassifier(max_depth=7)  \n",
    "fit_model(dt_7_2, \"Depth 7 Split 2\") \n",
    "\n",
    "dt_5_5 = DecisionTreeClassifier(max_depth=5, min_samples_split=5)  \n",
    "fit_model(dt_5_5, \"Depth 5 Split 5\") \n",
    "\n",
    "dt_7_5 = DecisionTreeClassifier(max_depth=7, min_samples_split=5)  \n",
    "fit_model(dt_7_5, \"Depth 7 Split 5\") \n",
    "\n",
    "dt_3_2 = DecisionTreeClassifier(max_depth=3, min_samples_split=2)  \n",
    "fit_model(dt_3_2, \"Depth 3 Split 2\") \n",
    "\n",
    "dt_3_5 = DecisionTreeClassifier(max_depth=3, min_samples_split=5)  \n",
    "fit_model(dt_3_5, \"Depth 3 Split 5\") \n",
    "\n",
    "dt_5_4 = DecisionTreeClassifier(max_depth=5, min_samples_split=4)  \n",
    "fit_model(dt_5_4, \"Depth 5 Split 4\") \n",
    "\n",
    "gnb = GaussianNB()\n",
    "fit_model(gnb, \"Naive Bayes\")\n",
    "\n",
    "kn = KNeighborsClassifier(n_neighbors=10)\n",
    "fit_model(kn, \"k-NN\")\n",
    "\n",
    "pipe_kn = Pipeline([(\"scaler\", MinMaxScaler()), (\"knr\", KNeighborsClassifier(n_neighbors=10))])\n",
    "fit_model(pipe_kn, \"Scaled k-NN\")\n",
    "\n",
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Simple Imputer\n",
    "\n",
    "df_test = pd.read_csv(\"dataset/test.csv\")\n",
    "\n",
    "df_test.drop(['RecordID', 'hospital_id', 'icu_id', 'icu_stay_type', 'ethnicity', 'gender', 'apache_3j_bodysystem', 'apache_2_bodysystem', 'icu_type', 'icu_admit_source'], axis=1, inplace=True)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# df_test['apache_3j_bodysystem'] = label_encoder.fit_transform(df_test['apache_3j_bodysystem'])\n",
    "# df_test['apache_2_bodysystem'] = label_encoder.fit_transform(df_test['apache_2_bodysystem'])\n",
    "# df_test['icu_type'] = label_encoder.fit_transform(df_test['icu_type'])\n",
    "# df_test['icu_admit_source'] = label_encoder.fit_transform(df_test['icu_admit_source'])\n",
    "\n",
    "\n",
    "df_test_onehot = pd.get_dummies(df_test)\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X_test = imputer.fit_transform(df_test_onehot)\n",
    "\n",
    "record_ids = np.arange(50001, 80001)\n",
    "\n",
    "\n",
    "predictions = dt_5_2.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "output_df = pd.DataFrame({'RecordID': record_ids, 'hospital_death': predictions})\n",
    "output_df.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Imputer\n",
    "\n",
    "df_test = pd.read_csv(\"dataset/test.csv\")\n",
    "\n",
    "\n",
    "df_test.drop(['RecordID', 'hospital_id', 'icu_id', 'icu_stay_type', 'ethnicity', 'gender'], axis=1, inplace=True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "df_test['apache_3j_bodysystem'] = label_encoder.fit_transform(df_test['apache_3j_bodysystem'])\n",
    "df_test['apache_2_bodysystem'] = label_encoder.fit_transform(df_test['apache_2_bodysystem'])\n",
    "df_test['icu_type'] = label_encoder.fit_transform(df_test['icu_type'])\n",
    "df_test['icu_admit_source'] = label_encoder.fit_transform(df_test['icu_admit_source'])\n",
    "\n",
    "df_test_onehot = pd.get_dummies(df_test)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=2, weights='uniform')\n",
    "X_test = imputer.fit_transform(df_test_onehot)\n",
    "\n",
    "record_ids = np.arange(50001, 80001)\n",
    "\n",
    "predictions = dt_7_5.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "output_df = pd.DataFrame({'RecordID': record_ids, 'hospital_death': predictions})\n",
    "output_df.to_csv('predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "df_test = pd.read_csv(\"dataset/test.csv\")\n",
    "\n",
    "# Drop columns that were removed during training\n",
    "columns_to_drop = ['RecordID', 'hospital_id', 'icu_id', 'icu_stay_type', 'ethnicity', 'gender']\n",
    "df_test_cleaned = df_test.drop(columns_to_drop, axis=1)\n",
    "\n",
    "# Ensure the order of columns is consistent with training data\n",
    "df_test_cleaned = df_test_cleaned[X.columns]\n",
    "df_test_naremoved = df_test_cleaned.dropna() \n",
    "\n",
    "# One-hot encode categorical columns\n",
    "df_test_onehot = pd.get_dummies(df_test_naremoved)\n",
    "\n",
    "# Generate 'RecordID' values from 50,001 to 80,000\n",
    "record_ids = np.arange(50001, 50001 + len(df_test_onehot))\n",
    "\n",
    "# Extract features (X_test) from the cleaned DataFrame\n",
    "X_test = df_test_onehot\n",
    "\n",
    "# Use the trained model to make predictions on the test dataset\n",
    "predictions = dt_5_2.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Create a DataFrame with 'RecordID' and 'hospital_death' columns\n",
    "output_df = pd.DataFrame({'RecordID': record_ids, 'hospital_death': predictions})\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "output_df.to_csv('predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
